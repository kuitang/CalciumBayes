\documentclass{article}
\usepackage{nips10submit_e,times}
%\documentstyle[nips07submit_09,times]{article}
\usepackage[square,numbers]{natbib}
\usepackage{amsmath, epsfig}
\usepackage{amsfonts}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{easybmat}
\usepackage{footmisc}
\renewcommand\algorithmiccomment[1]{// \textit{#1}}
%
\newcommand{\ignore}[1]{}
\newcommand{\comment}[1]{}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{Inferring Direct and Indirect Functional Connectivity Between Neurons From Multiple Neural Spike Train Data}

\author{
Ben Shababo \hspace{1cm} Kui Tang \hspace{1 cm}Conrad Stern-Ascher \hspace{1cm}Frank Wood\\
Columbia University, New York, NY 10027, USA \\
\texttt{\{bms2156,kt2384,cs3022\}@columbia.edu},
%\texttt{pfau@neurotheory.columbia.edu} 
\texttt{\{fwood\}@stat.columbia.edu} 
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\X}{\mathcal{X}}


\nipsfinalcopy

\begin{document}

\maketitle

\begin{abstract}
Our project aims to model the functional connectivity of neuronal
microcircuits. On this scale, we are concerned with how the activity
of each individual neuron relates to other neurons in the
population. Recent innovations, including the use of calcium indicator
dyes or multi-electrode arrays (MEA), allow researchers to collect
individual spiking activities of large groups of neighboring neurons,
supplying the data to address the fundamental problem of connectivity.
We will develop a model to infer a neural connectivity matrix given
spike train data that improves and builds on prior research efforts.
With a better understanding of the functional patterns of neural
activity at the cellular level, we can begin to decode the building
blocks of neural computation.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\subsection{Problem Description}

As we learn more and more about the workings of the neuron and of
specialized brain regions, the question increasingly becomes, how
do these pieces sum to a whole? How do the patterns of connectivity
give rise to vision, memory, motor function, and so on? Currently, a broad picture of the circuitry, or graphical connectivity,
of the brain does not exist, but several projects are underway to organize the solution of
this problem \citep{Marcus2011, Bohland2009}. Efforts to examine connectivity of the brain focus on scales ranging from brain regions each comprised of hundreds of
millions of cells down to microcircuits of only a few cells. Further,
some of these projects address structural connectivity and others
functional connectivity \citep{KnowlesBarley2011, Jain2010, Ropireddy2011, Chiang2011, bhattacharya2006}.

%We aim to produce a generative model of neuron spiking

In this project, we will focus on the functional connectivity of
microcircuits: how firing activity of one neuron influences the firing of other nearby neurons.
Importantly, functional connectivity does not always imply anatomical connectivity; it only implies that
some set of neurons fire together in correlation.  These jointly firing neurons may have a common input
or be linked in a chain, rather than having monosynaptic connection.

\subsection{Background}

Several strategies have already been employed to infer the functional
connectivity of microcircuits from calcium imaging and MEA data
\citep{Gerwinn2010, takahashi2007, aguiar2009}. Of special interest to us and our approach
are two recent Bayesian approaches. In \citep{patnaik2011}, 
a pattern-growth algorithm is used to find frequent patterns of firing 
activity. These patterns define mutual information between neurons
which they summarize in a dynamic Bayesian network. While their
methodology presents a contribution to the study of Bayesian networks,
one limitation of this work in inferring the connectivity of
microcircuits is that it only discovers relationships of excitation. In \citep{mishchencko2011}, network activity is modeled in terms of a collection of coupled hidden Markov chains, with each chain
corresponding to a single neuron in the network and the coupling
between the chains reflecting the networkâ€™s connectivity matrix.
To make computation feasible they used a blockwise-Gibbs sampling
method and took advantage of the parallel computing possibilities
when implementing their expectation-maximization algorithm. 

\subsection{Unobserved Neurons as Indirect Inputs}

Although the work to date has done much to address the problem of
functional neural connectivity, there are still improvements to be
made to current models. For example, current models do not address
unobserved inputs to the system. In this paper, we attempt to account for these indirect influences on the observed neurons by extending the model of \citep{mishchencko2011} so that it captures functional weights between neurons over multiple time steps, effectively extending the model back in time. In this way, we can capture many of the more interesting higher order patterns found in \citep{patnaik2011}.  But also, by allowing both positive and negative values for these weights we can capture inhibitory influences on these larger time courses.

\section{Methods}

\subsection{Formal Model}
We extend and synthesize a parametric generative model proposed by
\citep{mishchencko2011} of joint spike trains on $N$ neurons in
discrete time. Mischencko
et al. propose a model to infer the connectivity matrix $W$, where
each entry $w_{ij}$ encodes the influence of neuron $i$ on the subsequent
firing of neuron $j$, given the history of directly from calcium
flourescence data. Their model can be decomposed into one part
inferring neural spike train data from flourescence imaging, and
another part inferring $W$ from spike train data. We focus on the
latter.

Denote by $ n_i(t) $ whether neuron $i$ fired at time $t$. We observe
the firing of each neuron, $n_i(t), i = 1,...,N$, at each discrete time step, such that $n_i(t) = 1$ when we observe a spike and $n_i(t) = 0$ when the neuron is not firing. We model $n_i(t)$ as a
binary random variable with parameter $f(J_i(t))$, where
\begin{equation}\label{J}  J_i(t) = b_i + I_i(t) + \sum_{j=1}^{N}
w_{ij}h_{ij}(t), \end{equation} where $b_i$ is a baseline and $I_i(t)$
accounts for indirect influences on neuron $i$ from a fixed window of
past time steps. The history term, $h_{ij}$, encodes
the influence of neuron $j$ on neuron $i$ and is only dependent on firing of $j$ at time $t-\Delta$ where $\Delta$ is the size of each discrete time step.
From \citep{mishchencko2011}, we model $h_{ij}(t)$ as an autoregressive function: \begin{equation}\label{h} h_{ij}(t) = (1-\Delta/\tau_{ij}^h)h_{ij}(t-\Delta)
  + n_j(t-\Delta)+\sigma_{ij}^h\sqrt{\Delta}\epsilon_{ij}^h(t), \end{equation}
where $ \tau_{ij}^h $ is the decay time constant, $\sigma_{ij}^h$ is the
standard deviation of the noise and $\epsilon_{ij}^h$ is a standard
normal random variable representing noise.

Following \citep{mishchencko2011}, we define \begin{equation}
\label{f} f(J) = P\left(n>0 | n \sim \text{Poiss}(e^J\Delta)\right) = 1 - \exp(-e^J\Delta). \end{equation}

In addition, we attempt to model the indirect inputs to $n_i$ by summing the influences from all neurons, $n_j(s), s=t-2\Delta,...,t-S\Delta$, where $S$ is the temporal limit on indirect influences. These higher order interactions are incorporated into the summed input to neuron $n_i(t)$ by adding the term,

\begin{equation}
\label{new_term}
I_i(t)=\displaystyle\sum\limits_{s=2}^S\sum\limits_{j} \beta_{ijs}n_j(t-s\Delta) + \eta_{is},
\end{equation}


to the input function $J_i(t)$. Here, $s$ is the number of time steps back and $\beta_{ijs}$ is the weight of the indirect influence of $n_j(t-s\Delta)$ on $n_i(t)$. The term $\eta_{is}$ represents the indirect influence from unknown sources, i.e. influences not mediated through any of the $N$ observed neurons, on $n_i(t)$  Along with inferring the direct connectivity matrix, $W$, we will also infer the three dimensional indirect connectivity matrix, $B$, and the $N\times S-2$ matrix of $\eta_{is}$ terms.

\subsection{Priors}

In \citep{mishchencko2011}, they use two priors on the connectivity matrix, $W$, a sparseness prior and a prior which imposes "Dale's Law" which states that a neuron can only exert either an excitatory or an inhibitory influence on postsynaptic neurons. We will include the spareness prior in our model; however, given that recent research has shown that neurotransmitter co-release is more common that once anticipated, we will reexamine the effect of imposing Dale's Law on our model. We will also investigate imposing priors on the indirect influences. One possibility for getting a prior on excitatory indirect influences would be based on finding frequent episodes as in \citep{patnaik2011}.

\section{Experiments}

\subsection{Data Source \& Simulation}

We will test our model on both actual multiple neuronal spike train data as well as on simulated spike trains. The actual data have been provided by the Buszaki lab and was recorded simultaneously from 87 prefrontal cortex neurons of a behaving rat over the course of roughly 40 minutes. We also use simulated neuronal spike train data generated using equations (1)-(3) in \citep{mishchencko2011} which are similar to our equations \eqref{J}, \eqref{h}, and \eqref{f} except that instead of the term $I_i(t)$, they have a term representing some linearly filtered external stimulus, $k_iS^{ext}(t)$.

\subsection{Testing Method}

We will ensure the quality of our model and inference techniques on both types of data. As there is not ground-truth for the connection weights for actual data, we will be looking for weight values that create networks of connectivity that resemble those found in the graphical analysis of known neuronal circuits.

In the case of simulated data, we have an interesting opportunity to test the quality of the indirect component of our model. We can simulate a large collection neurons and only use a subset as the observed neurons in our model. We can then compare our inferred indirect weights with the known direct weights of the simulated data.

\section{Conclusion}
We will develop a model to infer a neural connectivity matrix given spike train data (either "continuous" measurements, such as a fluorescence or electrophysiology time series, or pre-processed discrete spike times) that improves and builds on prior research efforts. As ground truth data is limited in this domain, we will primarily measure performance using the same simulation processes and metrics used in the source papers for our model.  Specifically, we expect our model to accurately retrieve the connectivity parameters used to generate the simulated data.  

Further, since our team has access to limited lab data of spike trains, we will also compare our model against existing models on real data and expect our model to outperform on real data as well.  A potential metric for performance in this regime may come from the few experiments done, where dozens of cells are voltage clamped simultaneously, and functional connections are accurately teased out through direct measurements of post synaptic currents.  Certain graphical features have been inferred about the entire microcircuit from these limited electrophysiological data, such as the existence of scale-free, small world networks with certain two, three, and four cell motifs over-represented as compared to a randomly connected network \citep{song2005,perin2011}.  So in comparison to other models we may expect to validate these graphical features in the analysis of real data.

\begin{small}
\bibliographystyle{plainnat}
\bibliography{refs} 
\end{small}
\end{document}
