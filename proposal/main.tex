\documentclass{article}
\usepackage{nips10submit_e,times}
%\documentstyle[nips07submit_09,times]{article}
\usepackage[square,numbers]{natbib}
\usepackage{amsmath, epsfig}
\usepackage{amsfonts}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{easybmat}
\usepackage{footmisc}
\renewcommand\algorithmiccomment[1]{// \textit{#1}}
%
\newcommand{\ignore}[1]{}
\newcommand{\comment}[1]{}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{Inferring Direct and Indirect Functional Connectivity Between Neurons From Multiple Neural Spike Train Data}

\author{
Ben Shababo \hspace{1cm} Kui Tang \hspace{1 cm}Frank Wood\\
Columbia University, New York, NY 10027, USA \\
\texttt{\{bms2156,kt2384\}@columbia.edu},
%\texttt{pfau@neurotheory.columbia.edu} 
\texttt{\{fwood\}@stat.columbia.edu} 
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\X}{\mathcal{X}}


\nipsfinalcopy

\begin{document}

\maketitle

\begin{abstract}
Our project aims to model the functional connectivity of neural
microcircuits. On this scale, we are concerned with how the activity
of each individual neuron relates to other nearby neurons in the
population. Though several models and methods have been implemented to infer neural microcircuit connectivity, these fail to capture unobserved influences on the microcircuit. In this paper, we address these hidden influences on the microcircuit by developing a model which takes into account functional connectivity between observed neurons over more than one time step as well as inputs to each neuron which are unmediated by the observed neurons. We then test this model by simulating a large population of neurons, but only observing a subpopulation which allows us to compare our inferred indirect connectivity with the known direct connectivity of the total population. With a better understanding of the functional patterns of neural
activity at the cellular level, we can begin to decode the building
blocks of neural computation.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\subsection{Problem Description}

As we learn more and more about the workings of the neuron and of
specialized brain regions, the question increasingly becomes, how
do these pieces sum to a whole? How do the patterns of connectivity
give rise to vision, memory, motor function, and so on? Currently, a broad picture of the circuitry, or graphical connectivity,
of the brain does not exist, but several projects are underway to organize the solution of
this problem \citep{Marcus2011, Bohland2009}. Efforts to examine connectivity of the brain focus on scales ranging from brain regions each comprised of hundreds of
millions of cells down to microcircuits of only a few cells. Further,
some of these projects address structural connectivity and others
functional connectivity \citep{KnowlesBarley2011, Jain2010, Ropireddy2011, Chiang2011, bhattacharya2006}.

%We aim to produce a generative model of neuron spiking

In this project, we will focus on the functional connectivity of
microcircuits: how firing activity of one neuron influences the firing of other nearby neurons.
Importantly, functional connectivity does not always imply anatomical connectivity; it only implies that
some set of neurons fire together in correlation.  These jointly firing neurons may have a common input
or be linked in a chain, rather than having monosynaptic connection.

\subsection{Background}

Several strategies have already been employed to infer the functional
connectivity of microcircuits from calcium imaging and MEA data
\citep{Gerwinn2010, takahashi2007, aguiar2009}. Of special interest to us and our approach
are two recent Bayesian approaches. In \citep{patnaik2011}, 
a pattern-growth algorithm is used to find frequent patterns of firing 
activity. These patterns define mutual information between neurons
which they summarize in a dynamic Bayesian network. While their
methodology presents a contribution to the study of Bayesian networks,
one limitation of this work in inferring the connectivity of
microcircuits is that it only discovers relationships of excitation. In \citep{mishchencko2011}, network activity is modeled in terms of a collection of coupled hidden Markov chains, with each chain
corresponding to a single neuron in the network and the coupling
between the chains reflecting the networkâ€™s connectivity matrix.
To make computation feasible they used a blockwise-Gibbs sampling
method and took advantage of the parallel computing possibilities
when implementing their expectation-maximization algorithm. 

\subsection{Unobserved Neurons as Indirect Inputs}

Although the work to date has done much to address the problem of
functional neural connectivity, there are still improvements to be
made to current models. For example, current models do not address
unobserved inputs to the system. In this paper, we attempt to account for these indirect influences on the observed neurons by extending the model of \citep{mishchencko2011} so that it captures functional weights between neurons over multiple time steps, effectively extending the model back in time. In this way, we can capture many of the more interesting higher order patterns found in \citep{patnaik2011}.  But also, by allowing both positive and negative values for these weights we can capture excitatory and inhibitory influences on these larger time courses.

\section{Methods}

\subsection{Formal Model}
We extend and synthesize a parametric generative model proposed by
\citep{mishchencko2011} of joint spike trains on $N$ neurons in
discrete time. Mischencko
et al. propose a model to infer the connectivity matrix $W$, where
each entry $w_{ij}$ encodes the influence of neuron $j$ on the subsequent
firing of neuron $i$, given the history of directly from calcium
florescence data. Their model can be decomposed into one part
inferring neural spike train data from florescence imaging, and
another part inferring $W$ from spike train data. We focus on the
latter.

Denote by $ n_i(t) $ whether neuron $i$ fired at time $t$. We observe
the firing of each neuron, $n_i(t), i = 1,...,N$, at each discrete time step, such that $n_i(t) = 1$ when we observe a spike and $n_i(t) = 0$ when the neuron is silent. We model $n_i(t)$ as a
Bernoulli random variable with parameter $f(J_i(t))$, where
\begin{equation}\label{J}J_i(t) = b_i + I_i(t) + \sum_{j=1}^{N}
w_{ij}h_{ij}(t), \end{equation} where $b_i$ is a baseline and $I_i(t)$
accounts for indirect influences on neuron $i$ from a fixed window of
past time steps. The history term, $h_{ij}$, encodes
the influence of neuron $j$ on neuron $i$ and is only dependent on the firing of $j$ at time $t-\Delta$ where $\Delta$ is the size of each discrete time step.
From \citep{mishchencko2011}, we model $h_{ij}(t)$ as an autoregressive function: \begin{equation}\label{h} h_{ij}(t) = (1-\Delta/\tau_{ij}^h)h_{ij}(t-\Delta)
  + n_j(t-\Delta)+\sigma_{ij}^h\sqrt{\Delta}\epsilon_{ij}^h(t), \end{equation}
where $ \tau_{ij}^h $ is the decay time constant, $\sigma_{ij}^h$ is the
standard deviation of the noise and $\epsilon_{ij}^h$ is a standard
normal random variable representing noise.

Following \citep{mishchencko2011}, we define \begin{equation}
\label{f} f(J) = P\left(n>0 | n \sim \text{Poiss}(e^J\Delta)\right) = 1 - \exp(-e^J\Delta), \end{equation}

which completes the setup for the generalized linear model. 

In addition, we attempt to model the indirect inputs to $n_i$ by summing the influences from all neurons, $n_j(t-s\Delta), s=2,...,S$, where $S$ is the temporal limit on indirect influences. These higher order interactions are incorporated into the summed input to neuron $n_i(t)$ by adding the term,

\begin{equation}
\label{new_term}
I_i(t)=\displaystyle\sum\limits_{s=2}^S\sum\limits_{j=1}^N \beta_{ijs}n_j(t-s\Delta) + \lambda_{is},
\end{equation}


to the input function $J_i(t)$. Here, $s$ is the number of time steps back and $\beta_{ijs}$ is the weight of the indirect influence of $n_j(t-s\Delta)$ on $n_i(t)$. The term $\lambda_{is}$ represents the indirect influence from unknown sources, i.e., influences not mediated through any of the $N$ observed neurons, on $n_i(t)$.  Along with inferring the direct connectivity matrix, $W$, we will also infer the three dimensional indirect connectivity matrix, $B$, and the $N\times S-2$ matrix of $\lambda_{is}$ terms.

\subsection{Priors}

In \citep{mishchencko2011}, they use two priors on the connectivity matrix, $W$, a sparseness prior and a prior which imposes "Dale's Law" which states that a neuron can only exert either an excitatory or an inhibitory influence on postsynaptic neurons. We will include the sparseness prior in our model; however, given that recent research has shown that neurotransmitter co-release is more common that once anticipated, we will reexamine the effect of imposing Dale's Law on our model. We will also investigate imposing priors on the indirect influences. One possibility for a prior on excitatory indirect influences would be based on finding frequent episodes as in \citep{patnaik2011}.

\section{Experiments}

\subsection{Data Source \& Simulation}

We will test our model on actual multiple neuronal spike train data as well as on simulated spike trains. The actual data have been provided by the Buszaki Lab and was recorded simultaneously from 87 prefrontal cortex neurons of a behaving rat over the course of roughly 40 minutes. This data has been pre-processed so that we begin with the spike trains which are equivalent to the values of $n_i(t)$.

We also use simulated neuronal spike train data generated using equations (1)-(3) in \citep{mishchencko2011} which are similar to our equations \eqref{J}, \eqref{h}, and \eqref{f} except that instead of the term $I_i(t)$, they have a term representing some linearly filtered external stimulus, $k_i \cdot S^{ext}(t)$.

\subsection{Testing Method}

We will ensure the quality of our model and inference techniques on both types of data. As there is not ground-truth for the connection weights for actual data, we will be looking for weight values that reveal network motifs that resemble those found in the graphical analysis of known neuronal circuits such as the scale-free, small world networks with certain two, three, and four cell motifs \citep{song2005,perin2011}.

In the case of simulated data, we have an interesting opportunity to test the quality of the indirect component of our model. We can simulate a large collection neurons and only use a subset as the observed neurons in our model. We can then compare our inferred indirect weights with the known direct weights of the simulated data.

\section{Expected Conclusions \& Results}
We propose to develop and test a model for inferring the direct and indirect functional connectivity of a neural microcircuit given discrete multiple neuronal spike train data. As ground truth data is limited in this domain, we will primarily measure performance using the same simulation processes and metrics used in the source papers for our model.  Specifically, we expect our model to accurately retrieve the connectivity parameters used to generate the simulated data. We go a step further by comparing our inferred parameters for indirect influences on the observed neurons to known chains of direct connectivity which pass through unobserved neurons.

Further, since we have access to actual lab spike train data, we will also compare our model against existing models on real data.  A potential metric for performance in this regime may come from the few experiments done, where dozens of cells are voltage clamped simultaneously, and functional connections are accurately teased out through direct measurements of post synaptic currents.  Also, we expect to validate the existence of graphical features commonly associated with neural circuitry.

\begin{small}
\bibliographystyle{plainnat}
\bibliography{refs} 
\end{small}
\end{document}
